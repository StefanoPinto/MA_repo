---
title: "Polarization Metrics Overview"
author: "Stefano"
date: "2025-02-19"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("C:/Users/Stef/Desktop/Uni/CU/semester_4/ma")
```

```{r}
library(tidyverse)
library(naniar)
library(countrycode)
library(psych)
library(weights)
```

# Data

To do: Short description of the data

```{r}
data <- read_csv("data2.csv")
```

## Cleaning

- **TO DO**: Add explanations what 77, 88, 99 mean (something like "don't" know, "refuse", etc.)

```{r}
select_and_clean <- function(dataset) {
  dataset_clean <- dataset |> 
    select(essround, cntry, imbgeco, imueclt, imwbcnt, anweight, pspwght, pweight) |> 
    replace_with_na(replace = list(
      imbgeco = c(77, 88, 99),
      imueclt = c(77, 88, 99),
      imwbcnt = c(77, 88, 99))) |> 
    mutate(essround = 2000 + 2 * essround) |> 
    na.omit()
    
  return(dataset_clean)
}

```

```{r}
data <- read_csv("data2.csv")
data <- select_and_clean(data)
```

## Including weight

- We want to use `anweight` as weighting variable
- In some rounds, `anweight` is missing - but we can easily calculate it by multiplying `pspwght` with `pweight` (https://www.europeansocialsurvey.org/sites/default/files/2023-06/ESS_weighting_data_1_1.pdf page 6)

```{r}
data <- data |> 
  mutate(anweight = pspwght * pweight)
```

```{r}
# 0 
sum(is.na(data$anweight))
```

## Fixing countrycodes

```{r}
data <- data |> 
  mutate(cntry = countrycode(sourcevar = cntry,
                             origin = "iso2c",
                             destination = "country.name.en"),
         .keep = "unused")

data[which(is.na(data$cntry)), "cntry"] <- "Kosovo"
```

#  Response fractions

- *imbgeco*, *imueclt* and *imwbcnt* all range from 0 (extremely negative) to 10 (extremely positive), with 5 being a neutral stance
- $p_0, p_1, ... p_9, p_{10}$ denote the **fractions** of valid answers for the options zero to ten regarding the item in question

# Polarization metrics from "Did Europeans’ Opinions on Immigration Polarize? A Cross-national Study of Time-trends (Draft)"

## #0) Weighted ratios

- We need to calculate the **weighted** ratios $p_0$, $p_1$, $p_2$ etc. several times, so we write a function

```{r}
weighted_ratios <- function(dataset, variable) {
  
  if(nrow(dataset) == 0) {
    return(NA)
  }
  # calculate weighted ratios
  weighted_counts <- dataset %>%
    group_by({{ variable }}) %>%
    summarise(weighted_n = sum(anweight)) 
  
  available_rounds <- pull(unique(weighted_counts[,1]))
  
  n_total_weighted <- sum(weighted_counts)
  
  weighted_ratios <- weighted_counts$weighted_n / n_total_weighted
  names(weighted_ratios) <- available_rounds
  
  # Returns a vector of n ratios / probabilities, one for each available round
  return(weighted_ratios)
}
```

## #1) Non-neutrality

- The fraction of individuals who do not use the central (neutral) answer option (five on the eleven-point scale from zero to ten).
- This assesses polarization in a very basic sense by counting the fraction of people who do not take a neutral stance
- **Interpretation**: For public opinion on immigration increased non-neutrality means that more individuals take a decision to either lean towards acceptance or objection instead of being impartial. Most polarized in that sense would be a society without any impartial opinions. It captures loss of central mass.


$$
\textrm{Non-neutrality} = 1-p_5 \ 
$$

```{r}
non_neutrality <- function(dataset, variable) {
  
  if(nrow(dataset) == 0) {
    return(NA)
  }
  
  all_voters_count <- dataset |> select(anweight) |> sum()

# voters who gave a 5 (weighted)
  neutrals_count <- dataset |>
    filter({{ variable }} == 5) |>
    select(anweight) |> sum(na.rm = T)

  # ratio of neutrals to all voters
  ratio = neutrals_count / all_voters_count

  non_neutrality <- 1 - ratio

  return(non_neutrality)
  }

non_neutrality(data, imueclt)

```

## #2) Average deviation from neutrality

- Shows resemblance to the concept of group polarization in psychology: How far is the average attitude away from the scale’s midpoint?
- This is operationalized by capturing the absolute value of the distance of the mean of the opinion distribution
- For public opinion on immigration an increasing average deviation from neutrality means that individuals become either increasingly accepting or increasingly rejecting in their views. Most polarized in that sense would be a society where everyone has an extreme stance, be it "0" or "10".
- It also relates to the concept of group polarization from social psychology as it captures a shift of opinion positions “toward a more extreme point” (cf. Sunstein, 2003, p. 81). However, the most polarized society in this notion is also consensual in its extremity, and thus least polarized in the following notion.


$$
\textrm{Average deviation from neutrality} = \frac{1}{5}|\mu-5| \\ \text{where } \mu=\sum_{i=0}^{10} i \cdot p_i
$$


```{r}
avg_deviation_from_neutrality <- function(dataset, variable) {
  
  if(nrow(dataset) == 0) {
    return(NA)
  }
  
  # get the weighted ratios of (if available) p0 to p10
  weighted_ratios <- weighted_ratios(dataset, {{ variable }})
  
  # calculate weighted expected value (mu)
  rounds <- map_int(names(weighted_ratios), \(n) as.integer(n))
  mu <- sum(map2_dbl(weighted_ratios, rounds, \(ratio, round) ratio * round))

  # calculate average deviation from neutrality (5)
  # 0 is neutral, 1 is max deviation
  avg_deviation <- (1/5) * (abs(mu - 5))
 
  return(avg_deviation)

}

```

## #3) Dispersion

- Is measured by its mean absolute deviation.
- This is a good basic measure of polarization for typical survey response distributions on bounded scales (0-10 in the ESS).
- The maximum dispersion is when half of the individuals are on both extremes while the minimal dispersion appears for any consensus with all individuals having the same response. This measure is exactly as proposed by Bramson et al. (2016)

$$
\textrm{Dispersion} = \frac 1 5 \sum_{i=0}^{10} p_i \cdot |i-\mu|
$$


- The factor $\frac 1 5$ normalizes the measure to achieve its maximal possible value at 1.
- For public opinion on immigration increasing dispersion means that individuals deviate more from the average attitude. Most polarized would be a society divided into two equally sized groups of people advocating total acceptance and total objection.

```{r}
dispersion <- function(dataset, variable) {
  
  if(nrow(dataset) == 0) {
    return(NA)
  }
  
  # get the weighted ratios of p0 to p10
  weighted_ratios <- weighted_ratios(dataset, {{ variable }})
  
  # calculate weighted expected value (mu)
  rounds <- map_int(names(weighted_ratios), \(n) as.integer(n))
  
  mu <- sum(map2_dbl(weighted_ratios, rounds , \(ratio, round) ratio * round))

  dispersion <- (1/5) * sum(weighted_ratios * abs(rounds - mu))
  
  return(dispersion)
}
```

## #4) Moderate divergence

- Assessed by the absolute difference of group means of the moderate accepting group and the moderate opposing group, as described in Bramson et al. (2016).
- Lorenz (2017) analyzed the typical characteristics of ESS opinion distributions and pointed to the existence of five endogenous groups in ESS opinion distributions:
  - The extreme left
  - The moderate left
  - The neutrals
  - The moderate right
  - The extreme right
- Per item, we operationalize similar groups, consisting of:
  - The "full-on acceptors": Individuals with opinion **0**
  - The "moderate accepting group": Individuals with opinion **1-4**
  - The "neutrals": Individuals with opinion **5**
  - The "moderate opposing group": Individuals with opinion **6-9**
  - The "full-on opponents": Individuals with opinion **10**


$$
\textrm{Moderate divergence} = \frac{1}{10}|\mu_{m[oppose]}-\mu_{m[accept]}| \\ \textrm{with } \\ p_{m[accecpt]} = p_1 + p_2 + p_3 + p_4 \\
\textrm{and} \\
p_{m[oppose]} = p_6 + p_7 + p_8 + p_9 \\
\textrm{and} \\
\mu_{m[accept]} = \frac{1 \cdot p_1 +2 \cdot p_2 + 3 \cdot p_3 + 4 \cdot p_4}{p_{m[accepct]}} \\
\textrm{and} \\
\mu_{m[oppose]} = \frac{6 \cdot p_6 + 7 \cdot p_7 + 8 \cdot p_8 + 9 \cdot p_9}{p_{m[oppose]}}
$$

- The factor $\frac{1}{10}$ normalizes the measure to range from 0 to 1 (0 = maximally similar, 1 = maximally divergent)

-  Starting from here, p1 to p4 and p6 to p9 MUST be present, because those are the moderate groups

```{r}
find_moderate_values <- function(ratios) {
  
  moderate_values <- c("1", "2", "3", "4", "6", "7", "8", "9")
  indices <- na.omit(as.numeric(map(moderate_values, \(val) which(names(ratios) == val))))
  
  if(length(indices) == 8) {
    return(ratios[indices])
  }
  else {
    return(NA)
  }
}
```

```{r}
moderate_divergence <- function(dataset, variable) {
  
  if(nrow(dataset) == 0) {
    return(NA)
  }
  
  ratios <- weighted_ratios(dataset, {{ variable }})
  
  # Check if p1 to p4 and p6 to p9 are available
  ratios <- find_moderate_values(ratios)
  
  if(any(is.na(ratios))) {
    return(NA)
  }
  
  p_m_accept <- ratios[1:4] # p1 to p4
  p_m_oppose <- ratios[5:8] # p6 to p9

  mu_m_accept <- sum(map2_dbl(p_m_accept, 1:4, \(x,y) x * y)) / (sum(p_m_accept))
  mu_m_oppose <- sum(map2_dbl(p_m_oppose, 6:9, \(x,y) x * y)) / (sum(p_m_oppose))
  
  mod_divergence <- (1/10) * (abs(mu_m_accept - mu_m_oppose))
  return(mod_divergence)
  }

```

## #5) Moderate group consensus

- Based on the mean absolute deviation (MAD) of the two moderate groups. In contrast to dispersion, which we assess as MAD of the entire opinion distribution, the measurement of group consensus increases with decreasing dispersion in the two groups.

$$
\textrm{Moderate group consensus} = 1 - \frac 1 2(MAD_{m[accept]} + MAD_{m[oppose]}) \\
\textrm{where} \\
MAD_{m[accept]} = \sum_{i=1}^{4} \frac{p_j \cdot |i-\mu_{m[accept]}|}{\sum_{j=1}^{4}p_j} \\
\textrm{and} \\
MAD_{m[oppose]} = \sum_{i=6}^{9} \frac{p_j \cdot |i-\mu_{m[oppose]}|}{\sum_{j=6}^{9}p_j}
$$

```{r}
moderate_group_consensus <- function(dataset, variable) {
  
  if(nrow(dataset) == 0) {
    return(NA)
  }
  
  ratios <- weighted_ratios(dataset, {{ variable }})
  
  # Check if p1 to p4 and p6 to p9 are available
  ratios <- find_moderate_values(ratios)
  
  if(any(is.na(ratios))) {
    return(NA)
  }
  
  p_m_accept <- ratios[1:4] # p1 to p4
  p_m_oppose <- ratios[5:8] # p6 to p9

  mu_m_accept <- sum(map2_dbl(p_m_accept, 1:4, \(x,y) x * y)) / (sum(p_m_accept))
  mu_m_oppose <- sum(map2_dbl(p_m_oppose, 6:9, \(x,y) x * y)) / (sum(p_m_oppose))

  mad_m_accept <- sum(map_dbl(1:4, \(i) ((p_m_accept[i] * abs(i - mu_m_accept)) / sum(p_m_accept))))
  mad_m_oppose <- sum(map_dbl(6:9, \(i) ((p_m_oppose[i-5] * abs(i - mu_m_oppose)) / sum(p_m_oppose))))

  mgc <- 1 - (1/2) * (mad_m_accept * mad_m_oppose)
  return(mgc)
}
```

## #6) Moderate size parity

- The relative size of the smaller group of moderates compared to the larger group. Hereby the mass of the smaller group is divided by the mass of the larger group.
- A size parity of 1 indicates equal size of moderate groups and thereby the maximum possible polarization in the sense of parity. This is a simplified version of the size parity measure proposed by Bramson et al. (2016).

$$
\textrm{Moderate size parity} = min \lbrace \frac{p_{m[accept]}}{p_{m[oppose]}}, \frac{p_{m[oppose]}}{p_{m[accept]}} \rbrace
$$

- For public opinion on immigration, increasing moderate size parity means that moderately accepting views and moderately opposing views become more similar in numbers. Most polarized in that sense would be a society where both groups are of equal size

```{r}
moderate_size_parity <- function(dataset, variable) {
  
  if(nrow(dataset) == 0) {
    return(NA)
  }
  
  ratios <- weighted_ratios(dataset, {{ variable }})
  
  # Check if p1 to p4 and p6 to p9 are available
  ratios <- find_moderate_values(ratios)
  
  if(any(is.na(ratios))) {
    return(NA)
  }
  
  p_m_accept <- ratios[1:4] # p1 to p4
  p_m_oppose <- ratios[5:8] # p6 to p9

  accept_over_oppose <- p_m_accept / p_m_oppose
  oppose_over_accept <- p_m_oppose / p_m_accept
  
  msp <- min(accept_over_oppose, oppose_over_accept)
  return(msp)
}
```

## Building the country-round-variable combinations

```{r}
variables <- c("imbgeco", "imueclt", "imwbcnt")
countries = unique(data$cntry)
essrounds = unique(data$essround)

combinations_df <- expand_grid(variable = variables, country = countries, essround = essrounds)

```

```{r}
functions <- c(
  non_neutrality,
  avg_deviation_from_neutrality,
  dispersion,
  moderate_divergence,
  moderate_group_consensus,
  moderate_size_parity)

calculate_metric <- function(dataset, func) {
  
  return_values <- c()
  
  for(i in 1:nrow(combinations_df)) {
    variable <- combinations_df[i,]$variable
    country <- combinations_df[i,]$country
    round <- combinations_df[i,]$essround
  
    sub_df <- dataset |> filter(essround == round, cntry == country)
    
    tryCatch(
      {
        val <- func(sub_df, !!sym(variable))
        return_values[i] <- val
        },
      error = function(e) {print(e)},
      warning = function(w) {print(c(variable, country, round))},
      message = function(m) {print(m)},
      finally = {print(sub_df)}
    )
  }
  return(return_values)
}
```

```{r}
results <- map(functions, \(func) calculate_metric(data, func))
```

```{r}
metrics <- c(
  "non_neutrality",
  "avg_deviation_from_neutrality",
  "dispersion",
  "moderate_divergence",
  "moderate_group_consensus",
  "moderate_size_parity")

for(i in 1:length(metrics)) {
  combinations_df[metrics[i]] <- results[[i]]
}
```

```{r}
write_csv(combinations_df, "metrics.csv")
```


# Multi-dimensional approach from Stefano's capstone project

- Short descriptions using PCA, linear regression etc.

- The main idea is to create the correlation matrix using the weights, resulting in the weighted correlation matrix based on which we do the PCA

```{r}
weighted_pca <- function(dataset) {
  
  weights <- dataset$anweight
  
  sub_df <- dataset |> 
  select(imbgeco, imueclt, imwbcnt) |> 
  map_df(\(var) var - weighted.mean(var, weights)) |>  
  map_df(\(var) var / sqrt(wtd.var(var, weights))) 

  weighted_cor_matrix <- wtd.cors(sub_df, weight = weights)

  pca_result <- princomp(covmat = weighted_cor_matrix, cor = T) 
  
  return(pca_result)
}
```

## Round - Country combinations for PCA

```{r}
pca_df <- expand_grid(country = unique(data$cntry), essround = unique(data$essround))
```

```{r}
pca_list <- vector("list", nrow(pca_df))

for(i in 1:nrow(pca_df)) {
  
  country <- pca_df[i,]$country
  round <- pca_df[i,]$essround
  
  sub_df <- data |> filter(cntry == country, essround == round)
  
  tryCatch(
    {pca_list[[i]] <- weighted_pca(sub_df)},
    error = function(e) {pca_list[[i]] <- NA},
    finally = {print(sub_df)}
  )
}
```

## Functions for the PCA objects

```{r}
get_explained_variance_pc1 <- function(pca_obj) {
  
  if(is.null(pca_obj)) {
    return(NA)
  }
  
  explained_variance <- pca_obj$sdev^2 / sum(pca_obj$sdev^2)
  names(explained_variance) <- c("PC1", "PC2", "PC3")
  
  explained_variance_df <- explained_variance |> 
  as.data.frame() |> 
  rownames_to_column("PC") |> 
  as_tibble() |> 
  filter(PC == "PC1") |> 
  pull(explained_variance)
  
  return(explained_variance_df)
}
```

```{r}
expl_var_pc1 <- map_dbl(pca_list, get_explained_variance_pc1)
```

```{r}
# SLOPES DF
```

```{r}
get_loadings <- function(pca_obj) {
  
  if(is.null(pca_obj)) {
    return(NA)
  }
  
  var_loadings <- pca_obj$loadings[1:3,]
  
  loadings_pc1 <- var_loadings |> 
  as.data.frame() |> 
  rownames_to_column("variable") |> 
  as_tibble() |> 
  rename(PC1 = Comp.1, PC2 = Comp.2, PC3 = Comp.3) |> 
  pull(PC1)
  
  return(loadings_pc1)
}
```

```{r}
get_loadings(pca_list[[1]])
```

```{r}
# LOADINGS DF
```

